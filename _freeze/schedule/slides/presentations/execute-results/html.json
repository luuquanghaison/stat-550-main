{
  "hash": "2e9ce329d64b0dc019ba6255dd443ec8",
  "result": {
    "markdown": "---\nlecture: \"Model selection and overfitting\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\noutput-file: html\n---\n# {{< meta lecture >}}\nStat 550\n\n{{< meta author >}}\n\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n\n## Structure\n\n<br>\n\n\n1. Model selection\n\n<br>\n\n\n2. Underfitting and overfitting\n\n<br>\n\n3. Validation set and cross-validation\n\n\n\n# Model selection {background-color=\"#86D4FE\"}\n\n\n## Statistical model\n\nRepresents assumptions about data generation process\n\nOften includes parameters to be estimated\n\n* Linear: $y = ax + b + \\epsilon$\n\n* Logistic: $P(y=0) = \\frac{1}{1+e^{ax+b}}$\n\n* `...`\n\n\n. . .\n\nThe real world is too complicated\n\n. . .\n\n$\\Rightarrow$ All models are wrong\n\n. . .\n\n$\\Rightarrow$ Choose the best fitting one\n\n\n## \n<center>\n\n![](gfx/curve_fitting.png){width=\"73%\"}\n\n</center>\n\n\n## Model fit\n\n**Q**: How do you quantify \"best\" fit?\n\n<br>\n\n. . .\n\n**A**: By the [mean squared error]{.secondary} (MSE) between estimated and true value for new data\n\nFor prediction\n$$\\text{MSE} = E(\\hat{y}-y)^2$$\nFor inference\n$$\\text{MSE} = E(\\hat{\\theta}-\\theta)^2$$\n\n# Underfitting and Overfitting {background-color=\"#86D4FE\"}\n\n## Bias-variance breakdown\nMSE can be broken down into\n\n<br>\n\n$$\\begin{align}\n  \\text{MSE} = \\underbrace{Var(\\hat{y})}_{\\text{Variance}} + \\underbrace{(E(\\hat{y})-y)^2}_{\\text{Bias}^2} + \\underbrace{\\sigma^2}_{\\text{Noise}}\n\\end{align}$$\n\n* [Bias]{.secondary}: error due to model not being able to capture the truth\n* [Variance]{.secondary}: error due to sensitivity to noise in data\n\n. . .\n\nProblem: Bias and variance terms depend on true value which is unknown at training time\n\n## Underfitting and overfitting\n\nModel complexity affects both bias and variance\n\n* [Underfitting]{.secondary}: \n  - Model too simple to capture the underlying truth\n  - Estimate tends to have high bias, low variance\n\n* [Overfitting]{.secondary}: \n  - Model too complex leading to noise sensitivity\n  - Estimate tends to have low bias, high variance\n  \n. . .\n  \n[Overfitting]{.secondary} is harder to detect and therefore more problematic\n\n## Bias-variance trade-off {.nostretch}\n<center>\n\n![](gfx/b_vs_v.png){width=\"90%\" height=800}\n<center/>\n\n## Bias-variance trade-off (cont) {.nostretch}\n<center>\n\n![](gfx/right_model.png){width=\"90%\" height=750}\n<center/>\n\n\n# Validation set & cross-validation {background-color=\"#86D4FE\"}\n\n## Validation set\n\nA part of the data used to evaluate model performance before applying to new data\n\n<br>\n\nProcedure\n\n1. Randomly split the data into [train]{.secondary} set and [validation]{style=\"color:blue;\"} set\n\n2. Fit model using [train]{.secondary} set\n\n3. Evaluate model fit using [validation]{style=\"color:blue;\"} set\n\n4. Use the evaluation to inform hyperparameter choices\n\n5. Fit the final model on **both** sets using the model with best validation fit\n\n## Cross-validation\n\nProblem: a single validation set may not represent the data well\n\n. . .\n\nSolution: \n\n* Do it again with a different split\n\n* Use average performance across splits as model fit\n\n. . .\n\n[K-fold]{.secondary}: use 1/k th of the data as validation each split\n\n[Leave one out]{.secondary}: use 1 data point as validation each split\n\n## Visualization\n\n<center>\n\n![](gfx/K_fold.png){width=\"85%\"}\n\n<center/>\n\n\n## Takeaways\n\n<br>\n\nModels should balance between bias and variance for best fit\n\n<br>\n\nValidation set can help evaluate model performance\n\n<br>\n\nCross-validation averages performance across different splits for better consistency\n\n## References\n<br>\n\n_Points of significance: model selection and overfitting_, Jake Lever, Martin Krzywinski, Naomi Altman (2016), Nature methods.\n\n<br>\n\n_An introduction to statistical learning_, Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani (2013), Springer\n\n<br>\n\n_The Elements of Statistical Learning_, Trevor Hastie, Robert Tibshirani, Jerome Friedman (2009), Springer",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    // dispatch for htmlwidgets\r\n    function fireSlideEnter() {\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n    }\r\n\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n      fireSlideEnter();\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}