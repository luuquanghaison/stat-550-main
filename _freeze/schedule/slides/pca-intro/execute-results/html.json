{
  "hash": "b43f7074da22613bdd9c0a2aefd2fc3d",
  "result": {
    "markdown": "---\nlecture: \"Principal components analysis\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n## {{< meta lecture >}} {.large background-image=\"img/consult.jpeg\" background-opacity=\"0.3\"}\n\n[Stat 550]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 06 February 2024\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n\n## Representation learning\n\nRepresentation learning is the idea that performance of ML methods is\nhighly dependent on the choice of representation\n\n\nFor this reason, much of ML is geared towards transforming the data into\nthe relevant features and then using these as inputs\n\n\nThis idea is as old as statistics itself, really,\n\nHowever, the idea is constantly revisited in a variety of fields and\ncontexts\n\n\nCommonly, these learned representations capture low-level information\nlike overall shapes\n\n\n\nIt is possible to quantify this intuition for PCA at least\n\n. . .\n\nGoal\n: Transform $\\mathbf{X}\\in \\R^{n\\times p}$ into $\\mathbf{Z} \\in \\R^{n \\times ?}$\n\n?-dimension can be bigger (feature creation) or smaller (dimension reduction) than $p$\n\n\n\n\n\n## PCA\n\nPrincipal components analysis (PCA) is a dimension\nreduction technique\n\n\nIt solves various equivalent optimization problems\n\n(Maximize variance, minimize $\\ell_2$ distortions, find closest subspace of a given rank, $\\ldots$)\n\nAt its core, we are finding linear combinations of the original\n(centered) data $$z_{ij} = \\alpha_j^{\\top} x_i$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](pca-intro_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center'}\n:::\n:::\n\n\n\n\n## Lower dimensional embeddings\n\nSuppose we have predictors $\\x_1$ and $\\x_2$ (columns / features / measurements)\n\n-   We more faithfully preserve the structure of this data by keeping\n    $\\x_1$ and setting $\\x_2$ to zero than the opposite\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](pca-intro_files/figure-revealjs/unnamed-chunk-2-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Lower dimensional embeddings\n\nAn important feature of the previous example is that $\\x_1$ and $\\x_2$\naren't correlated\n\nWhat if they are?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](pca-intro_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='center'}\n:::\n:::\n\n\nWe lose a lot of structure by setting either $\\x_1$ or $\\x_2$ to zero\n\n\n\n## Lower dimensional embeddings\n\n\nThe only difference is the first is a rotation of the second\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](pca-intro_files/figure-revealjs/unnamed-chunk-4-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## PCA\n\nIf we knew how to rotate our data, then we could more \neasily retain the structure.\n\n[PCA]{.secondary} gives us exactly this rotation\n\n1. Center (+scale?) the data matrix $\\X$\n2. Compute the SVD of $\\X = \\U\\D \\V^\\top$ (always exists)\n3. Return $\\U_M\\D_M$, where $\\D_M$ is the largest $M$\n    singular values of $\\X$\n\n\n## PCA\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](pca-intro_files/figure-revealjs/unnamed-chunk-5-1.svg){fig-align='center'}\n:::\n:::\n\n\n## PCA on some pop music data\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,269 × 15\n   artist      danceability energy   key loudness  mode speechiness acousticness\n   <fct>              <dbl>  <dbl> <int>    <dbl> <int>       <dbl>        <dbl>\n 1 Taylor Swi…        0.781  0.357     0   -16.4      1      0.912       0.717  \n 2 Taylor Swi…        0.627  0.266     9   -15.4      1      0.929       0.796  \n 3 Taylor Swi…        0.516  0.917    11    -3.19     0      0.0827      0.0139 \n 4 Taylor Swi…        0.629  0.757     1    -8.37     0      0.0512      0.00384\n 5 Taylor Swi…        0.686  0.705     9   -10.8      1      0.249       0.832  \n 6 Taylor Swi…        0.522  0.691     2    -4.82     1      0.0307      0.00609\n 7 Taylor Swi…        0.31   0.374     6    -8.46     1      0.0275      0.761  \n 8 Taylor Swi…        0.705  0.621     2    -8.09     1      0.0334      0.101  \n 9 Taylor Swi…        0.553  0.604     1    -5.30     0      0.0258      0.202  \n10 Taylor Swi…        0.419  0.908     9    -5.16     1      0.0651      0.00048\n# ℹ 1,259 more rows\n# ℹ 7 more variables: instrumentalness <dbl>, liveness <dbl>, valence <dbl>,\n#   tempo <dbl>, time_signature <int>, duration_ms <int>, explicit <lgl>\n```\n:::\n:::\n\n\n## PCA on some pop music data\n\n* 15 dimensions to 2\n* coloured by artist\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](pca-intro_files/figure-revealjs/pca-music-plot-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Plotting the weights, $\\alpha_j,\\ j=1,2$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](pca-intro_files/figure-revealjs/unnamed-chunk-6-1.svg){fig-align='center'}\n:::\n:::\n\n\n# Mathematical details\n\n## Matrix decompositions\n\nAt its core, we are finding linear combinations of the original\n(centered) data $$z_{ij} = \\alpha_j^{\\top} x_i$$\n\n\nThis is expressed via the SVD: $\\X  = \\U\\D\\V^{\\top}$.\n\n\n::: {.callout-important}\nWe assume throughout that we have centered the data\n:::\n\nThen our new features are\n\n$$\\mathbf{Z} = \\X \\V = \\U\\D$$\n\n\n\n## Short SVD aside \n\n* Any $n\\times p$ matrix can be decomposed into $\\mathbf{UDV}^\\top$.\n\n* This is a computational procedure, like inverting a matrix,  `svd()`\n\n* These have properties:\n\n1. $\\mathbf{U}^\\top \\mathbf{U} = \\mathbf{I}_n$\n2. $\\mathbf{V}^\\top \\mathbf{V} = \\mathbf{I}_p$\n3. $\\mathbf{D}$ is diagonal (0 off the diagonal)\n\n\n[Many]{.secondary} methods for dimension reduction use the SVD of some matrix.\n\n\n\n## Why? {.smaller}\n\n1. Given $\\X$, find a projection $\\mathbf{P}$ onto $\\R^M$ with $M \\leq p$ \nthat minimizes the reconstruction error\n$$\n\\begin{aligned}\n\\min_{\\mathbf{P}} &\\,\\, \\lVert \\mathbf{X} - \\mathbf{X}\\mathbf{P} \\rVert^2_F \\,\\,\\, \\textrm{(sum all the elements)}\\\\\n\\textrm{subject to} &\\,\\, \\textrm{rank}(\\mathbf{P}) = M,\\, \\mathbf{P} = \\mathbf{P}^T,\\, \\mathbf{P} = \\mathbf{P}^2\n\\end{aligned}\n$$\nThe conditions ensure that $\\mathbf{P}$ is a projection matrix onto $M$ dimensions.\n\n2. Maximize the variance explained by an orthogonal transformation $\\mathbf{A} \\in \\R^{p\\times M}$\n$$\n\\begin{aligned}\n\\max_{\\mathbf{A}} &\\,\\, \\textrm{trace}\\left(\\frac{1}{n}\\mathbf{A}^\\top \\X^\\top \\X \\mathbf{A}\\right)\\\\\n\\textrm{subject to} &\\,\\, \\mathbf{A}^\\top\\mathbf{A} = \\mathbf{I}_M\n\\end{aligned}\n$$\n\n* In case one, the minimizer is $\\mathbf{P} = \\mathbf{V}_M\\mathbf{V}_M^\\top$\n* In case two, the maximizer is $\\mathbf{A} = \\mathbf{V}_M$.\n\n## Code output to look at\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n",
    "supporting": [
      "pca-intro_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}