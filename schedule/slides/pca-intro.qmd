---
lecture: "Principal components analysis"
format: revealjs
metadata-files: 
  - _metadata.yml
---

{{< include _titleslide.qmd >}}


## Representation learning

Representation learning is the idea that performance of ML methods is
highly dependent on the choice of representation


For this reason, much of ML is geared towards transforming the data into
the relevant features and then using these as inputs


This idea is as old as statistics itself, really,

However, the idea is constantly revisited in a variety of fields and
contexts


Commonly, these learned representations capture low-level information
like overall shapes



It is possible to quantify this intuition for PCA at least

. . .

Goal
: Transform $\mathbf{X}\in \R^{n\times p}$ into $\mathbf{Z} \in \R^{n \times ?}$

?-dimension can be bigger (feature creation) or smaller (dimension reduction) than $p$





## PCA

Principal components analysis (PCA) is a dimension
reduction technique


It solves various equivalent optimization problems

(Maximize variance, minimize $\ell_2$ distortions, find closest subspace of a given rank, $\ldots$)

At its core, we are finding linear combinations of the original
(centered) data $$z_{ij} = \alpha_j^{\top} x_i$$

```{r}
#| echo: false
#| dev: png
mat1 <- matrix(sample(letters, 200, replace = TRUE), 20, 10)
mat2 <- rbind(
  matrix(NA, 10, 2),
  matrix(sample(letters, 20, replace = TRUE), 10, 2)
)
mat3 <- matrix(sample(letters, 40, replace = TRUE), 20, 2)


plist <- map(list(mat3, mat1, mat2), ~ {
  ggplot(as_tibble(.x) |> mutate(row = 1:n()) |> pivot_longer(-row)) +
    geom_raster(aes(name, row, fill = value)) +
    scale_fill_viridis_d() +
    coord_fixed() +
    theme_void() +
    theme(legend.position = "none")
})

cowplot::plot_grid(plotlist = plist, ncol = 3, labels = c("", "=", "x"), 
                   label_size = 40, label_y = 0.75, label_x = c(0, -.33, 0))
```



## Lower dimensional embeddings

Suppose we have predictors $\x_1$ and $\x_2$ (columns / features / measurements)

-   We more faithfully preserve the structure of this data by keeping
    $\x_1$ and setting $\x_2$ to zero than the opposite

```{r}
#| echo: false
#| fig-width: 8
df <- tibble(
  x1 = rep(rnorm(20, sd = 1), 3),
  x2 = rep(rnorm(20, sd = .2), 3),
  lab = rep(c("data", "x2 only", "x1 only"), each = 20)
)
df[21:40, 1] <- 0
df[41:60, 2] <- 0
ggplot(df, aes(x1, x2, color = lab)) +
  geom_point() +
  coord_cartesian(c(-2, 2), c(-2, 2)) +
  scale_color_manual(values = c(blue, orange, green)) +
  facet_wrap(~lab) +
  theme(legend.title = element_blank(), legend.position = "none")
```


## Lower dimensional embeddings

An important feature of the previous example is that $\x_1$ and $\x_2$
aren't correlated

What if they are?

```{r}
#| echo: false
#| fig-width: 8
theta <- -pi / 4
R <- matrix(c(cos(theta), sin(theta), -sin(theta), cos(theta)), 2)
X <- as.matrix(df[1:20, 1:2]) %*% R
df2 <- data.frame(
  x1 = rep(X[, 1], 3),
  x2 = rep(X[, 2], 3),
  lab = rep(c("data", "x2 only", "x1 only"), each = 20)
)
df2[21:40, 1] <- 0
df2[41:60, 2] <- 0
ggplot(df2, aes(x1, x2, color = lab)) +
  geom_point() +
  scale_color_manual(values = c(blue, orange, green)) +
  coord_cartesian(c(-2, 2), c(-2, 2)) +
  facet_wrap(~lab) +
  theme(legend.title = element_blank(), legend.position = "none")
```

We lose a lot of structure by setting either $\x_1$ or $\x_2$ to zero



## Lower dimensional embeddings


The only difference is the first is a rotation of the second

```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 4
df3 <- bind_rows(df, df2)
df3$version <- rep(c("original", "rotated"), each = 60)
df3 %>%
  ggplot(aes(x1, x2, color = lab)) +
  geom_point() +
  scale_color_manual(values = c(blue, orange, green)) +
  coord_cartesian(c(-2, 2), c(-2, 2)) +
  facet_grid(~version) +
  theme(legend.title = element_blank(), legend.position = "bottom")
```


## PCA

If we knew how to rotate our data, then we could more 
easily retain the structure.

[PCA]{.secondary} gives us exactly this rotation

1. Center (+scale?) the data matrix $\X$
2. Compute the SVD of $\X = \U\D \V^\top$ (always exists)
3. Return $\U_M\D_M$, where $\D_M$ is the largest $M$
    singular values of $\X$


## PCA

```{r}
#| code-fold: true
#| fig-width: 10
#| fig-height: 4
s <- svd(X)
tib <- rbind(X, s$u %*% diag(s$d), s$u %*% diag(c(s$d[1], 0)))
tib <- tibble(
  x1 = tib[, 1], x2 = tib[, 2],
  name = rep(1:3, each = 20)
)
plotter <- function(set = 1, main = "original") {
  tib |>
    filter(name == set) |>
    ggplot(aes(x1, x2)) +
    geom_point(colour = blue) +
    coord_cartesian(c(-2, 2), c(-2, 2)) +
    theme(legend.title = element_blank(), legend.position = "bottom") +
    ggtitle(main)
}
cowplot::plot_grid(
  plotter() + labs(x = bquote(x[1]), y = bquote(x[2])),
  plotter(2, "rotated") +
    labs(x = bquote((UD)[1] == (XV)[1]), y = bquote((UD)[2] == (XV)[2])),
  plotter(3, "rotated+projected") +
    labs(x = bquote(U[1] ~ D[1] == (XV)[1]), y = bquote(U[2] ~ D[2] %==% 0)),
  nrow = 1, axis = "tb"
)
```

## PCA on some pop music data

```{r pca-music}
#| code-fold: true
music <- Stat406::popmusic_train
X <- music |> select(danceability:energy, loudness, speechiness:valence)
pca <- prcomp(X, scale = TRUE) ## DON'T USE princomp()
music
```

## PCA on some pop music data

* 15 dimensions to 2
* coloured by artist

```{r pca-music-plot}
#| code-fold: true
proj_pca <- predict(pca)[,1:2] |>
  as_tibble() |>
  mutate(artist = music$artist)
#| fig-width: 8
#| fig-height: 3
g1 <- ggplot(proj_pca, aes(PC1, PC2, color = artist)) +
  geom_point() +
  theme(legend.position = "none") +
  scale_color_viridis_d()
g2 <- ggplot(
  tibble(var_explained = pca$sdev^2 / sum(pca$sdev^2), M = 1:ncol(X)),
  aes(M, var_explained)
) +
  geom_point(color = orange) +
  scale_x_continuous(breaks = 1:10) +
  geom_segment(aes(xend = M, yend = 0), color = blue)
cowplot::plot_grid(g1, g2)
```


## Plotting the weights, $\alpha_j,\ j=1,2$

```{r}
#| code-fold: true
#| fig-width: 10
#| fig-height: 5
pca$rotation[, 1:2] |>
  as_tibble() |>
  set_names(c("Weight 1", "Weight 2")) |>
  mutate(feature = names(X)) |>
  pivot_longer(-feature) |>
  ggplot(aes(value, feature, fill = feature)) +
  facet_wrap(~name) +
  geom_col() +
  theme(legend.position = "none", axis.title = element_blank()) +
  scale_fill_brewer(palette = "Dark2") +
  geom_vline(xintercept = 0)
```

# Mathematical details

## Matrix decompositions

At its core, we are finding linear combinations of the original
(centered) data $$z_{ij} = \alpha_j^{\top} x_i$$


This is expressed via the SVD: $\X  = \U\D\V^{\top}$.


::: {.callout-important}
We assume throughout that we have centered the data
:::

Then our new features are

$$\mathbf{Z} = \X \V = \U\D$$



## Short SVD aside 

* Any $n\times p$ matrix can be decomposed into $\mathbf{UDV}^\top$.

* This is a computational procedure, like inverting a matrix,  `svd()`

* These have properties:

1. $\mathbf{U}^\top \mathbf{U} = \mathbf{I}_n$
2. $\mathbf{V}^\top \mathbf{V} = \mathbf{I}_p$
3. $\mathbf{D}$ is diagonal (0 off the diagonal)


[Many]{.secondary} methods for dimension reduction use the SVD of some matrix.



## Why? {.smaller}

1. Given $\X$, find a projection $\mathbf{P}$ onto $\R^M$ with $M \leq p$ 
that minimizes the reconstruction error
$$
\begin{aligned}
\min_{\mathbf{P}} &\,\, \lVert \mathbf{X} - \mathbf{X}\mathbf{P} \rVert^2_F \,\,\, \textrm{(sum all the elements)}\\
\textrm{subject to} &\,\, \textrm{rank}(\mathbf{P}) = M,\, \mathbf{P} = \mathbf{P}^T,\, \mathbf{P} = \mathbf{P}^2
\end{aligned}
$$
The conditions ensure that $\mathbf{P}$ is a projection matrix onto $M$ dimensions.

2. Maximize the variance explained by an orthogonal transformation $\mathbf{A} \in \R^{p\times M}$
$$
\begin{aligned}
\max_{\mathbf{A}} &\,\, \textrm{trace}\left(\frac{1}{n}\mathbf{A}^\top \X^\top \X \mathbf{A}\right)\\
\textrm{subject to} &\,\, \mathbf{A}^\top\mathbf{A} = \mathbf{I}_M
\end{aligned}
$$

* In case one, the minimizer is $\mathbf{P} = \mathbf{V}_M\mathbf{V}_M^\top$
* In case two, the maximizer is $\mathbf{A} = \mathbf{V}_M$.

## Code output to look at


```{r}
#| eval: false
pca <- prcomp(X) # never use `princomp()`
predict(pca)[,1:2] # the 2D embedding
pca$x[,1:2] # also the 2D embedding
pca$rotation[,1:2] # the weights
pca$sdev^2 / sum(pca$sdev^2) # % variance explained
```
